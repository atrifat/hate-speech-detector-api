{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --user scipy nltk imbalanced_learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n",
    "! unzip jigsaw-unintended-bias-in-toxicity-classification.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['id','comment_text','target']\n",
    "total_data = pd.read_csv(\"./train.csv\",usecols=selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1804874 entries, 0 to 1804873\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   id            int64  \n",
      " 1   target        float64\n",
      " 2   comment_text  object \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 41.3+ MB\n"
     ]
    }
   ],
   "source": [
    "total_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97320 entries, 0 to 97319\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   id            97320 non-null  int64  \n",
      " 1   comment_text  97320 non-null  object \n",
      " 2   toxicity      97320 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "selected_columns = ['id','comment_text','toxicity']\n",
    "final_test_data = pd.read_csv(\"./test_public_expanded.csv\",usecols=selected_columns)\n",
    "final_test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import [Contractions](https://stackoverflow.com/a/19794953/8141330) for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./contractions.json', 'r') as f:\n",
    "    contractions_dict = json.load(f)\n",
    "contractions = contractions_dict['contractions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function which handles emoji classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :') , :O\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\)|:O)', ' positiveemoji ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positiveemoji ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' positiveemoji ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-; , @-)\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;|@-\\))', ' positiveemoji ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:, :-/ , :-|\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:|:-/|:-\\|)', ' negativeemoji ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negativeemoji ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function which handles all preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()  # Lowercases the string\n",
    "    tweet = re.sub(\"@[^\\s]+\", \"\", tweet)  # Removes usernames\n",
    "    tweet = re.sub(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \" \", tweet)  # Remove URLs\n",
    "    tweet = re.sub(r\"\\d+\", \" \", str(tweet))  # Removes all digits\n",
    "\n",
    "    for word in tweet.split():\n",
    "        if word.lower() in contractions:\n",
    "            tweet = tweet.replace(\n",
    "                word, contractions[word.lower()]\n",
    "            )  # Replaces contractions\n",
    "\n",
    "    tweet = re.sub(\"&quot;\", \" \", tweet)  # Remove (&quot;)\n",
    "    tweet = emoji(tweet)  # Replaces Emojis\n",
    "    tweet = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(tweet))  # Removes all single characters\n",
    "\n",
    "    tweet = re.sub(r\"[^\\w\\s]\", \" \", str(tweet))  # Removes all punctuations\n",
    "    tweet = re.sub(\n",
    "        r\"(.)\\1+\", r\"\\1\\1\", tweet\n",
    "    )  # Convert more than 2 letter repetitions to 2 letter\n",
    "    tweet = re.sub(r\"\\s+\", \" \", str(tweet))  # Replaces double spaces with single space\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text using process_tweet function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data['processed_text'] = np.vectorize(process_tweet)(total_data[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data['processed_text'] = np.vectorize(process_tweet)(final_test_data[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "english_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "vectorizer = CountVectorizer(stop_words=english_stop_words)\n",
    "# vectorizer = TfidfVectorizer(use_idf=True,ngram_range=(1,1), max_features=20000, stop_words=english_stop_words)\n",
    "vectorizer.fit(total_data['processed_text'])\n",
    "\n",
    "processed_text_vector = vectorizer.transform(total_data['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1804874x261309 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 42463570 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save vectorizer for future use as picke file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vectorizer_count_no_stop_words.pkl', 'wb') as fout:\n",
    "    pickle.dump(vectorizer, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vectorizer from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vectorizer_count_no_stop_words.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text_vector = vectorizer.transform(total_data['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text_final_vector = vectorizer.transform(final_test_data['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform target label to binary label, 1=toxic, 0=non-toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target label to binary label, 1=toxic, 0=non-toxic\n",
    "total_data[\"label\"] = np.where(total_data[\"target\"] > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target label to binary label, 1=toxic, 0=non-toxic\n",
    "final_test_data[\"label\"] = np.where(final_test_data[\"toxicity\"] > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_text_vector, total_data[\"label\"],\n",
    "                                                    test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set dataset for final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = processed_text_final_vector\n",
    "y_test_final = final_test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset Using Random Undersample and Oversample Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "random_oversample = RandomOverSampler(random_state=69,sampling_strategy='minority')\n",
    "X_train_oversample, y_train_oversample = random_oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "random_undersample = RandomUnderSampler(random_state=69,sampling_strategy='majority')\n",
    "X_train_undersample, y_train_undersample = random_undersample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "model_lr = LogisticRegression(C=1.0, random_state=69, solver='sag', max_iter=1000, n_jobs=-1).fit(X_train, y_train)\n",
    "predicted_lr = model_lr.predict(X_test)\n",
    "\n",
    "print(\"training time\",datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_lr_partial.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_lr, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_lr_partial.pkl', 'rb') as f:\n",
    "    model_lr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lr = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lr_final = model_lr.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report lr base\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    339536\n",
      "           1       0.76      0.45      0.56     21439\n",
      "\n",
      "    accuracy                           0.96    360975\n",
      "   macro avg       0.86      0.72      0.77    360975\n",
      "weighted avg       0.95      0.96      0.95    360975\n",
      "\n",
      "balanced_accuracy_score 0.7194135882344745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report lr base\")\n",
    "print(classification_report(y_test, predicted_lr))\n",
    "\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report lr base on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     91671\n",
      "           1       0.75      0.46      0.57      5649\n",
      "\n",
      "    accuracy                           0.96     97320\n",
      "   macro avg       0.86      0.72      0.77     97320\n",
      "weighted avg       0.96      0.96      0.96     97320\n",
      "\n",
      "balanced_accuracy_score 0.7241368992475128\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report lr base on final test set\")\n",
    "print(classification_report(y_test_final, predicted_lr_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_lr_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logistic Regression using Oversampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "model_lr_oversample = LogisticRegression(C=1.0, random_state=69, solver='sag', max_iter=1000, n_jobs=-1).fit(X_train_oversample, y_train_oversample)\n",
    "predicted_lr_oversample = model_lr_oversample.predict(X_test)\n",
    "\n",
    "print(\"training time\",datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_lr_partial_oversample.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_lr_oversample, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_lr_partial_oversample.pkl', 'rb') as f:\n",
    "    model_lr_oversample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lr_oversample = model_lr_oversample.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lr_oversample_final = model_lr_oversample.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Logistic Regression (Oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report lr oversample\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96    339536\n",
      "           1       0.42      0.83      0.56     21439\n",
      "\n",
      "    accuracy                           0.92    360975\n",
      "   macro avg       0.70      0.88      0.76    360975\n",
      "weighted avg       0.95      0.92      0.93    360975\n",
      "\n",
      "balanced_accuracy_score 0.880793107815532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report lr oversample\")\n",
    "print(classification_report(y_test, predicted_lr_oversample))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_lr_oversample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report lr overersample on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96     91671\n",
      "           1       0.42      0.84      0.56      5649\n",
      "\n",
      "    accuracy                           0.92     97320\n",
      "   macro avg       0.70      0.89      0.76     97320\n",
      "weighted avg       0.96      0.92      0.93     97320\n",
      "\n",
      "balanced_accuracy_score 0.8851825792799533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report lr overersample on final test set\")\n",
    "print(classification_report(y_test_final, predicted_lr_oversample_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_lr_oversample_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logistic Regression using Undersampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "model_lr_undersample = LogisticRegression(C=1.0, random_state=69, solver='sag', max_iter=1000, n_jobs=-1).fit(X_train_undersample, y_train_undersample)\n",
    "predicted_lr_undersample = model_lr_undersample.predict(X_test)\n",
    "\n",
    "print(\"training time\",datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_lr_partial_undersample.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_lr_undersample, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_lr_partial_undersample.pkl', 'rb') as f:\n",
    "    model_lr_undersample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lr_undersample = model_lr_undersample.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_lr_undersample_final = model_lr_undersample.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Logistic Regression (Undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report lr undersample\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95    339536\n",
      "           1       0.39      0.84      0.53     21439\n",
      "\n",
      "    accuracy                           0.91    360975\n",
      "   macro avg       0.69      0.88      0.74    360975\n",
      "weighted avg       0.95      0.91      0.93    360975\n",
      "\n",
      "balanced_accuracy_score 0.877678680785448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report lr undersample\")\n",
    "print(classification_report(y_test, predicted_lr_undersample))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_lr_undersample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report lr undersample on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95     91671\n",
      "           1       0.39      0.84      0.53      5649\n",
      "\n",
      "    accuracy                           0.91     97320\n",
      "   macro avg       0.69      0.88      0.74     97320\n",
      "weighted avg       0.95      0.91      0.93     97320\n",
      "\n",
      "balanced_accuracy_score 0.879569566970637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report lr undersample on final test set\")\n",
    "print(classification_report(y_test_final, predicted_lr_undersample_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_lr_undersample_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier\n",
    "\n",
    "model_naive = MultinomialNB().fit(X_train, y_train)\n",
    "predicted_naive = model_naive.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_nb_partial.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_naive, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_nb_partial.pkl', 'rb') as f:\n",
    "    model_naive = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_naive = model_naive.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_naive_final = model_naive.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report nb base\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96    339536\n",
      "           1       0.42      0.50      0.46     21439\n",
      "\n",
      "    accuracy                           0.93    360975\n",
      "   macro avg       0.70      0.73      0.71    360975\n",
      "weighted avg       0.94      0.93      0.93    360975\n",
      "\n",
      "balanced_accuracy_score 0.7268720343393581\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report nb base\")\n",
    "print(classification_report(y_test, predicted_naive))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report nb base on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97     91671\n",
      "           1       0.47      0.48      0.48      5649\n",
      "\n",
      "    accuracy                           0.94     97320\n",
      "   macro avg       0.72      0.72      0.72     97320\n",
      "weighted avg       0.94      0.94      0.94     97320\n",
      "\n",
      "balanced_accuracy_score 0.7228788097322775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report nb base on final test set\")\n",
    "print(classification_report(y_test_final, predicted_naive_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_naive_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Naive Bayes (Oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier\n",
    "\n",
    "model_naive_oversample = MultinomialNB().fit(X_train_oversample, y_train_oversample)\n",
    "predicted_naive_oversample = model_naive_oversample.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_nb_partial_oversample.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_naive_oversample, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_nb_partial_oversample.pkl', 'rb') as f:\n",
    "    model_naive_oversample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_naive_oversample = model_naive_oversample.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_naive_oversample_final = model_naive_oversample.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Naive Bayes (Oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report nb random oversample\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.73      0.84    339536\n",
      "           1       0.16      0.86      0.28     21439\n",
      "\n",
      "    accuracy                           0.73    360975\n",
      "   macro avg       0.58      0.79      0.56    360975\n",
      "weighted avg       0.94      0.73      0.80    360975\n",
      "\n",
      "balanced_accuracy_score 0.7918526157110459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report nb random oversample\")\n",
    "print(classification_report(y_test, predicted_naive_oversample))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_naive_oversample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report nb oversample on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.73      0.84     91671\n",
      "           1       0.17      0.86      0.28      5649\n",
      "\n",
      "    accuracy                           0.74     97320\n",
      "   macro avg       0.58      0.80      0.56     97320\n",
      "weighted avg       0.94      0.74      0.81     97320\n",
      "\n",
      "balanced_accuracy_score 0.7966972454943804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report nb oversample on final test set\")\n",
    "print(classification_report(y_test_final, predicted_naive_oversample_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_naive_oversample_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Naive Bayes (Undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier\n",
    "\n",
    "model_naive_undersample = MultinomialNB().fit(X_train_undersample, y_train_undersample)\n",
    "predicted_naive_undersample = model_naive_undersample.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_nb_partial_undersample.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_naive_undersample, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_nb_partial_undersample.pkl', 'rb') as f:\n",
    "    model_naive_undersample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_naive_undersample = model_naive_undersample.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_naive_undersample_final = model_naive_undersample.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Naive Bayes (Undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report nb random undersample\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.71      0.83    339536\n",
      "           1       0.16      0.88      0.27     21439\n",
      "\n",
      "    accuracy                           0.72    360975\n",
      "   macro avg       0.58      0.80      0.55    360975\n",
      "weighted avg       0.94      0.72      0.80    360975\n",
      "\n",
      "balanced_accuracy_score 0.7950775960415505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report nb random undersample\")\n",
    "print(classification_report(y_test, predicted_naive_undersample))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_naive_undersample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report nb undersample on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.72      0.83     91671\n",
      "           1       0.16      0.88      0.27      5649\n",
      "\n",
      "    accuracy                           0.73     97320\n",
      "   macro avg       0.58      0.80      0.55     97320\n",
      "weighted avg       0.94      0.73      0.80     97320\n",
      "\n",
      "balanced_accuracy_score 0.7998241280455165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report nb undersample on final test set\")\n",
    "print(classification_report(y_test_final, predicted_naive_undersample_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_naive_undersample_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Balanced Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time 0:15:00.074344\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "model_brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=100, random_state=69, sampling_strategy=\"all\", replacement=True,\n",
    "    bootstrap=False, n_jobs=-1\n",
    ")\n",
    "model_brf.fit(X_train, y_train)\n",
    "\n",
    "print(\"training time\",datetime.datetime.now() - start)\n",
    "\n",
    "predicted_brf = model_brf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_brf_partial.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_brf, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_brf_partial.pkl', 'rb') as f:\n",
    "    model_brf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_brf = model_brf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_brf_final = model_brf.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Balanced Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report brf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96    339536\n",
      "           1       0.43      0.73      0.54     21439\n",
      "\n",
      "    accuracy                           0.93    360975\n",
      "   macro avg       0.71      0.84      0.75    360975\n",
      "weighted avg       0.95      0.93      0.93    360975\n",
      "\n",
      "balanced_accuracy_score 0.8355757192417335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report brf\")\n",
    "print(classification_report(y_test, predicted_brf))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_brf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report brf on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96     91671\n",
      "           1       0.43      0.73      0.54      5649\n",
      "\n",
      "    accuracy                           0.93     97320\n",
      "   macro avg       0.71      0.84      0.75     97320\n",
      "weighted avg       0.95      0.93      0.94     97320\n",
      "\n",
      "balanced_accuracy_score 0.8369791726680487\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report brf on final test set\")\n",
    "print(classification_report(y_test_final, predicted_brf_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_brf_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Balanced Random Forest (Undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time 0:13:42.828406\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "model_brf_undersample = BalancedRandomForestClassifier(\n",
    "    n_estimators=100, random_state=69, sampling_strategy=\"all\", replacement=True,\n",
    "    bootstrap=False, n_jobs=-1\n",
    ")\n",
    "model_brf_undersample.fit(X_train_undersample, y_train_undersample)\n",
    "\n",
    "print(\"training time\",datetime.datetime.now() - start)\n",
    "\n",
    "predicted_brf_undersample = model_brf_undersample.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_brf_partial_undersample.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_brf_undersample, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_brf_partial_undersample.pkl', 'rb') as f:\n",
    "    model_brf_undersample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_brf_undersample = model_brf_undersample.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Balanced Random Forest (Undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report brf random undersample\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93    339536\n",
      "           1       0.30      0.84      0.45     21439\n",
      "\n",
      "    accuracy                           0.88    360975\n",
      "   macro avg       0.65      0.86      0.69    360975\n",
      "weighted avg       0.95      0.88      0.90    360975\n",
      "\n",
      "balanced_accuracy_score 0.8602299095010775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report brf random undersample\")\n",
    "print(classification_report(y_test, predicted_brf_undersample))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_brf_undersample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Balanced Bagging Classifier (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time 0:38:06.360371\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_bbc_nb = BalancedBaggingClassifier(MultinomialNB(),\n",
    "                                sampling_strategy='auto',\n",
    "                                replacement=False,\n",
    "                                random_state=69)\n",
    "\n",
    "model_bbc_nb.fit(X_train, y_train)\n",
    "\n",
    "print(\"training time\",datetime.datetime.now() - start)\n",
    "\n",
    "predicted_bbc_nb = model_bbc_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_bbc_nb_partial.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_bbc_nb, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_bbc_nb_partial.pkl', 'rb') as f:\n",
    "    model_bbc_nb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_bbc_nb = model_bbc_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_bbc_nb_final = model_bbc_nb.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Balanced Bagging Classifier (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report bbc_nb\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.73      0.84    339536\n",
      "           1       0.17      0.87      0.28     21439\n",
      "\n",
      "    accuracy                           0.73    360975\n",
      "   macro avg       0.58      0.80      0.56    360975\n",
      "weighted avg       0.94      0.73      0.80    360975\n",
      "\n",
      "balanced_accuracy_score 0.798936298461086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report bbc_nb\")\n",
    "print(classification_report(y_test, predicted_bbc_nb))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_bbc_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report bbc_nb on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.73      0.84     91671\n",
      "           1       0.17      0.88      0.28      5649\n",
      "\n",
      "    accuracy                           0.74     97320\n",
      "   macro avg       0.58      0.80      0.56     97320\n",
      "weighted avg       0.94      0.74      0.81     97320\n",
      "\n",
      "balanced_accuracy_score 0.8037337515598814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report bbc_nb on final test set\")\n",
    "print(classification_report(y_test_final, predicted_bbc_nb_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_bbc_nb_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Random Undersample Boosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time 1:35:31.993989\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "model_rusboost = RUSBoostClassifier(n_estimators=200, algorithm='SAMME.R',\n",
    "                              random_state=69)\n",
    "model_rusboost.fit(X_train, y_train)\n",
    "\n",
    "print(\"training time\",datetime.datetime.now() - start)\n",
    "\n",
    "predicted_rusboost = model_rusboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_rusboost_partial.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_rusboost, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_rusboost_partial.pkl', 'rb') as f:\n",
    "    model_rusboost = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rusboost = model_rusboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rusboost_final = model_rusboost.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Random Undersample Boosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report rusboost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.95    339536\n",
      "           1       0.36      0.76      0.48     21439\n",
      "\n",
      "    accuracy                           0.90    360975\n",
      "   macro avg       0.67      0.84      0.72    360975\n",
      "weighted avg       0.95      0.90      0.92    360975\n",
      "\n",
      "balanced_accuracy_score 0.8361425444207731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report rusboost\")\n",
    "print(classification_report(y_test, predicted_rusboost))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_rusboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report rusboost on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95     91671\n",
      "           1       0.36      0.77      0.49      5649\n",
      "\n",
      "    accuracy                           0.91     97320\n",
      "   macro avg       0.67      0.84      0.72     97320\n",
      "weighted avg       0.95      0.91      0.92     97320\n",
      "\n",
      "balanced_accuracy_score 0.8425753876253297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report rusboost on final test set\")\n",
    "print(classification_report(y_test_final, predicted_rusboost_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_rusboost_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "model_voting_classifier = VotingClassifier(estimators=[\n",
    "    # ('model_naive', model_naive), \n",
    "    # ('model_naive_oversample', model_naive_oversample), \n",
    "    # ('model_naive_undersample', model_naive_undersample),\n",
    "    ('model_lr', model_lr),\n",
    "    ('model_lr_oversample', model_lr_oversample),\n",
    "    ('model_lr_undersample', model_lr_undersample),\n",
    "], voting='soft')\n",
    "model_voting_classifier.estimators_ = [\n",
    "    # model_naive, \n",
    "    # model_naive_oversample, \n",
    "    # model_naive_undersample, \n",
    "    model_lr,\n",
    "    model_lr_oversample,\n",
    "    model_lr_undersample,\n",
    "]\n",
    "\n",
    "model_voting_classifier.le_ = LabelEncoder().fit(y_train)\n",
    "model_voting_classifier.classes_ = model_voting_classifier.le_.classes_\n",
    "\n",
    "predicted_voting = model_voting_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_voting_partial_best.pkl', 'wb') as fout:\n",
    "    pickle.dump(model_voting_classifier, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_voting_partial_best.pkl', 'rb') as f:\n",
    "    model_voting_classifier = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_voting = model_voting_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_voting_final = model_voting_classifier.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Voting Classifier (NB, NB Oversample, NB Undersample, LR, LR Oversample, LR Undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report voting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96    339536\n",
      "           1       0.41      0.79      0.54     21439\n",
      "\n",
      "    accuracy                           0.92    360975\n",
      "   macro avg       0.70      0.86      0.75    360975\n",
      "weighted avg       0.95      0.92      0.93    360975\n",
      "\n",
      "balanced_accuracy_score 0.8579705335719856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report voting\")\n",
    "print(classification_report(y_test, predicted_voting))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_voting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report voting on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96     91671\n",
      "           1       0.42      0.79      0.55      5649\n",
      "\n",
      "    accuracy                           0.92     97320\n",
      "   macro avg       0.70      0.86      0.75     97320\n",
      "weighted avg       0.95      0.92      0.94     97320\n",
      "\n",
      "balanced_accuracy_score 0.8634181381497537\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report voting on final test set\")\n",
    "print(classification_report(y_test_final, predicted_voting_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_voting_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Voting Classifier (LR, LR Oversample, LR Undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report voting lr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97    339536\n",
      "           1       0.52      0.74      0.61     21439\n",
      "\n",
      "    accuracy                           0.94    360975\n",
      "   macro avg       0.75      0.85      0.79    360975\n",
      "weighted avg       0.96      0.94      0.95    360975\n",
      "\n",
      "balanced_accuracy_score 0.8480500151927538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report voting lr\")\n",
    "print(classification_report(y_test, predicted_voting))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_voting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report voting lr on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     91671\n",
      "           1       0.52      0.75      0.61      5649\n",
      "\n",
      "    accuracy                           0.95     97320\n",
      "   macro avg       0.75      0.85      0.79     97320\n",
      "weighted avg       0.96      0.95      0.95     97320\n",
      "\n",
      "balanced_accuracy_score 0.8532860327547032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report voting lr on final test set\")\n",
    "print(classification_report(y_test_final, predicted_voting_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_voting_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Detoxify (unbiased-small) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detoxify import Detoxify\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/unitaryai/detoxify/releases/download/v0.1.2/unbiased-albert-c8519128.ckpt\" to /root/.cache/torch/hub/checkpoints/unbiased-albert-c8519128.ckpt\n",
      "100%|| 44.6M/44.6M [00:01<00:00, 25.8MB/s]\n",
      "/root/miniconda3/envs/cuda11/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0960d0300541c09ea2d4c5305df2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a64b8816474ddabc8795816f37b0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cde06beb554481dafdc08f8e63e9ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e62ef28da6408c8330eef15cabc8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_detoxify = Detoxify(\"unbiased-small\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset for Detoxify Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(total_data[\"processed_text\"], total_data[\"label\"],\n",
    "                                                    test_size=0.2, random_state=69)\n",
    "X_test2_list = X_test2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2_final_list = final_test_data['processed_text'].tolist()\n",
    "y_test2_final = final_test_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using Test Set (Without Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detoxify_result_raw = []\n",
    "print(len(X_test2_list))\n",
    "for i in range(len(X_test2_list)):\n",
    "    temp_prediction = model_detoxify.predict(X_test2_list[i])\n",
    "    df = pd.DataFrame(temp_prediction,index=[0])\n",
    "    detoxify_result_raw.append(df[\"toxicity\"][0].round(3).astype(\"float\"))\n",
    "print(len(detoxify_result_raw))\n",
    "\n",
    "detoxify_result_raw_pd = pd.DataFrame(detoxify_result_raw)\n",
    "detoxify_result_raw_pd.to_csv('detoxify_result_raw_pd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using Final Test Set (Without Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detoxify_result_final_raw = []\n",
    "print(len(X_test2_list))\n",
    "for i in range(len(X_test2_list)):\n",
    "    temp_prediction = model_detoxify.predict(X_test2_list[i])\n",
    "    df = pd.DataFrame(temp_prediction,index=[0])\n",
    "    detoxify_result_final_raw.append(df[\"toxicity\"][0].round(3).astype(\"float\"))\n",
    "print(len(detoxify_result_final_raw))\n",
    "\n",
    "detoxify_result_final_raw_pd = pd.DataFrame(detoxify_result_final_raw)\n",
    "detoxify_result_final_raw_pd.to_csv('detoxify_result_final_raw_pd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "detoxify_result_raw_pd = pd.read_csv('detoxify_result_raw_pd.csv')\n",
    "detoxify_result = np.where(detoxify_result_raw_pd['0'] > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Report - Detoxify (unbiased-small) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report detoxify\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98    339536\n",
      "           1       0.63      0.79      0.70     21439\n",
      "\n",
      "    accuracy                           0.96    360975\n",
      "   macro avg       0.81      0.88      0.84    360975\n",
      "weighted avg       0.97      0.96      0.96    360975\n",
      "\n",
      "balanced_accuracy_score 0.8796666343029867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report detoxify\")\n",
    "print(classification_report(y_test, detoxify_result))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, detoxify_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report voting lr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97    339536\n",
      "           1       0.52      0.74      0.61     21439\n",
      "\n",
      "    accuracy                           0.94    360975\n",
      "   macro avg       0.75      0.85      0.79    360975\n",
      "weighted avg       0.96      0.94      0.95    360975\n",
      "\n",
      "balanced_accuracy_score 0.8480500151927538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report voting lr\")\n",
    "print(classification_report(y_test, predicted_voting))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test, predicted_voting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "detoxify_result_final_raw_pd = pd.read_csv('detoxify_result_final_raw_pd.csv')\n",
    "detoxify_result_final = np.where(detoxify_result_final_raw_pd['0'] > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report detoxify on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     91671\n",
      "           1       0.61      0.77      0.68      5649\n",
      "\n",
      "    accuracy                           0.96     97320\n",
      "   macro avg       0.80      0.87      0.83     97320\n",
      "weighted avg       0.96      0.96      0.96     97320\n",
      "\n",
      "balanced_accuracy_score 0.8688139637966112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report detoxify on final test set\")\n",
    "print(classification_report(y_test_final, detoxify_result_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, detoxify_result_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report voting lr on final test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     91671\n",
      "           1       0.52      0.75      0.61      5649\n",
      "\n",
      "    accuracy                           0.95     97320\n",
      "   macro avg       0.75      0.85      0.79     97320\n",
      "weighted avg       0.96      0.95      0.95     97320\n",
      "\n",
      "balanced_accuracy_score 0.8532860327547032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"report voting lr on final test set\")\n",
    "print(classification_report(y_test_final, predicted_voting_final))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"balanced_accuracy_score\", balanced_accuracy_score(y_test_final, predicted_voting_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have several experiments in toxicity classification using [Kaggle - Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data) dataset. We have conducted several experiments using traditional Machine Learning model: Logistic Regression, Logistic Regression with Undersampled Data, Logistic Regression with Oversampled Data, Naive Bayes, Naive Bayes with Oversampled Data, Naive Bayes with Undersampled Data, Balanced Random Forest, Bagging Naive Bayes Classifier, RUSBoosted Classifier, and Voting Classifier.\n",
    "\n",
    "Based on the experiments, best model (Voting Classifier Model using Logistic Regression + Undersample Logistic Regression Variant + Oversample Logistic Regression) can be achieved using traditional machine learning model with comparable performance with state of the art model (Detoxify) result as follows:\n",
    "- Voting Model achieved **0.95** in accuracy score while Detoxify (unbiased-small) achieve **0.96** in accurace score on final test set data.\n",
    "- Voting Model achieved **0.853** in balanced_accuracy score while Detoxify (unbiased-small) achieve **0.868** in balanced_accuracy score on final test set data.\n",
    "- Voting Model achieved **0.79** in macro average F1-score while Detoxify (unbiased-small) achieve **0.83** in macro average F1-score on final test set data.\n",
    "- Voting Model achieved **0.75** in recall score for toxic class (1) while Detoxify (unbiased-small) achieve **0.77** in balanced_accuracy score on final test set data.\n",
    "- **Recall score** for toxic class becomes **priority** due to the main focus of toxicity classification.\n",
    "- Various hyperparameters optimization, other preprocessing methods, and other Machine Learning and/or Deep Learning can be explored in other future experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Related Works\n",
    "\n",
    "- [Kaggle - Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data)\n",
    "- [Github - Detoxify](https://github.com/unitaryai/detoxify)\n",
    "- [Github (gunjannandy) - Twitter Sentiment Analysis](https://github.com/gunjannandy/twitter-sentiment-analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
